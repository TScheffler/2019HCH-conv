{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Approaches to Discourse on Social Media\n",
    "### by Tatjana Scheffler, University of Potsdam (tatjana.scheffler@uni-potsdam.de)\n",
    "\n",
    "Heidelberg Computational Humanities summer school<br/>\n",
    "Heidelberg<br/>\n",
    "July 16, 2019\n",
    "\n",
    "\n",
    "This assumes that you have a working Python 3 distribution (for example through Anaconda: https://www.continuum.io/downloads). You may have to install some packages that are used below.\n",
    "\n",
    "## Working with Tweets\n",
    "\n",
    "Importing a bunch of packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplejson as json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import nltk\n",
    "\n",
    "import langid\n",
    "\n",
    "# display option (don't cut off text in dataframe columns)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using a thread about hockey as an example here. You can find it at https://bit.ly/2YERjhD (The password should be known to you.)\n",
    "Here I'm loading the data from the output of a Twarc search of Twitter (a file of json objects):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "with open(\"cangertweets.json\") as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tweets.append(tweet)\n",
    "        \n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a dataframe\n",
    "df = pd.DataFrame(data = tweets)\n",
    "print(list(df))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Language Identification\n",
    "\n",
    "Most of the tweets are probably in English, but maybe there are some others?\n",
    "\n",
    "We can look at Twitter's own language identification. But note that it is notoriously bad (it seems to mostly reflect the *user's* profile language setting). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lang'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[~(df['lang'] == 'en')][['full_text','lang']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`langid` is a Python package that does language identification. It usually works quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langid.classify(df['full_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['langid_lang'] = df.apply(lambda x: langid.classify(x['full_text'])[0], axis=1)\n",
    "df['langid_lang'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[~(df['lang'] == df['langid_lang'])][['full_text','lang','langid_lang']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `langid` gets confused easily by single letters - in fact, only the name \"@KanadaBotschaft\" makes it change its opinion from English to German:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(langid.classify('@GermanyDiplo @TeamD @CanadaFP @GermanyInCanada @KanadaBotschaft Hot chocolate for everyone! ðŸ˜€ '))\n",
    "print(langid.classify('@GermanyDiplo @TeamD @CanadaFP @GermanyInCanada Hot chocolate for everyone! ðŸ˜€ '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could try and use this info to improve language identification a bit more... (How?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some ideas here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizing and Part of Speech Tagging Tweets\n",
    "\n",
    "### TweetNLP: \n",
    "\n",
    "TweetNLP is a standalone tokenizer and part of speech tagger, which you can run from the command line:\n",
    "http://www.cs.cmu.edu/~ark/TweetNLP/\n",
    "\n",
    "It takes a file with only tweet text as input, so we'll have to create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"hockeytweets.txt\",\"w\") \n",
    "for tweet in tweets:\n",
    "    text = tweet[\"full_text\"].replace('\\n',' ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "    outfile.write(text + '\\n')\n",
    "    \n",
    "outfile.close()\n",
    "\n",
    "hockeytweets = [line.strip() for line in open('hockeytweets.txt')]\n",
    "print (\"Number of tweets in hockey thread: \" + str(len(hockeytweets)))\n",
    "print (hockeytweets[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the tagger: \n",
    "\n",
    "`./runTagger.sh input > output`\n",
    "\n",
    "`./runTagger.sh --no-confidence hockeytweets.txt > hockeytweets-tagged.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hockeytweets_tagged = [line.strip() for line in open('hockeytweets-tagged.tsv')]\n",
    "\n",
    "tags_df = pd.read_csv('hockeytweets-tagged.tsv', sep='\\t', header=None, names=['tokens', 'tags','text'])\n",
    "\n",
    "tags_df\n",
    "#tags_df['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some other ways to look at tokenization / tagging\n",
    "\n",
    "Just for future reference, we'll skip ahead to the next part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DR)\n",
    "# Look at tokens and tags\n",
    "\n",
    "tokens, tags, text = hockeytweets_tagged[98].split('\\t')\n",
    "print(tokens)\n",
    "print(tags)\n",
    "\n",
    "ttags = tags.split(\" \")\n",
    "ttoks = tokens.split(\" \")\n",
    "for tok, tag in zip(ttags, ttoks):\n",
    "#    print(tok + \"\\t\" + tag + \"\\n\") # uncomment to see token - tag pairs\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a Python port of the Twitter tokenizer from TweetNLP which can be downloaded from here: https://github.com/myleott/ark-twokenize-py\n",
    "\n",
    "It allows us to use the tokenizer right in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twokenize import *\n",
    "\n",
    "tokenizeRawTweetText(\"@GermanyDiplo @TeamD @CanadaFP @GermanyInCanada @KanadaBotschaft Canada did, they beat the German woman's soccer team in the RIO Olympics.\")\n",
    "\n",
    "## Tokenize the 'full_text' column of our original dataframe and save the result into a new column 'text_tokens'\n",
    "# df['text_tokens_1'] = df.apply(lambda x: tokenizeRawTweetText(x['full_text']), axis=1)\n",
    "# df['text_tokens_1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Somajo / Someweta\n",
    "\n",
    "For German, you can use the tokenizer Somajo (https://github.com/tsproisl/SoMaJo) and POS tagger Someweta (https://github.com/tsproisl/SoMeWeTa). They can be used as Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from somajo import Tokenizer\n",
    "\n",
    "tok = Tokenizer(split_camel_case=True, token_classes=True, extra_info=True)\n",
    "\n",
    "tokenized_tweets = []\n",
    "\n",
    "for tweet in hockeytweets_tagged:\n",
    "    tokens, tags, text = tweet.split('\\t')\n",
    "    tokenized = tok.tokenize(text)\n",
    "    tokenized_tweets.append(tokenized)\n",
    "    \n",
    "print (tokenized_tweets[1977])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = tags_df['tokens']\n",
    "df['tags'] = tags_df['tags']\n",
    "df.head()[['full_text','tokens','tags','langid_lang']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text\n",
    "\n",
    "def remove_handles(tokens, tags):\n",
    "    tok_list = tokens.split(' ')\n",
    "    tag_list = tags.split(' ')\n",
    "    res = []\n",
    "    for tok, tag in zip(tok_list, tag_list):\n",
    "        if tag != '@': \n",
    "            if tag != 'U':\n",
    "                res.append(tok)\n",
    "    return(' '.join(res))\n",
    "\n",
    "remove_handles(df['tokens'][0],df['tags'][0])\n",
    "\n",
    "df['text_only'] = df.apply(lambda x: remove_handles(x['tokens'],x['tags']), axis=1)\n",
    "#df['text_only']\n",
    "\n",
    "def normalize_text(tokens, tags):\n",
    "    tok_list = tokens.split(' ')\n",
    "    tag_list = tags.split(' ')\n",
    "    res = []\n",
    "    for tok, tag in zip(tok_list, tag_list):\n",
    "        if tag == '@': \n",
    "            res.append('%USER%')\n",
    "        elif tag == 'U':\n",
    "            res.append('%URL%')\n",
    "        else:\n",
    "            res.append(tok)\n",
    "    return(' '.join(res))\n",
    "\n",
    "df['normalized_text'] = df.apply(lambda x: normalize_text(x['tokens'],x['tags']), axis=1)\n",
    "#df['normalized_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can recompute the language identification based only on the text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lang_new'] = df.apply(lambda x: langid.classify(x['text_only'])[0], axis=1)\n",
    "df['lang_new'].describe()\n",
    "#df.loc[~(df['lang_new'] == df['langid_lang'])][['full_text','lang_new','langid_lang']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language identification on very short tweets is difficult. One may want to rely on Twitter's own classification below a certain minimum length of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation structure\n",
    "\n",
    "We can visualize the entire conversation with the browser extension (Treeverse)[https://github.com/paulgb/Treeverse]. \n",
    "\n",
    "Let's build a discussion tree in Python connecting all the tweets to their replies (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitterconversations import *\n",
    "\n",
    "discussion_threads, replies_dict = make_discussions(tweets)\n",
    "\n",
    "print_discussions(discussion_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the tweet id as the dataframe index\n",
    "df.set_index('id_str',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a list of replies to each item\n",
    "\n",
    "df['replies'] = np.empty((len(df), 0)).tolist()  # create empty lists of direct replies\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    row['replies'] += replies_dict[index]\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Now we can for example find all tweets that contain questions and their answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df[df['full_text'].str.contains('\\?')]\n",
    "questions[['full_text','replies']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many questions have answers?\n",
    "print(\"Questions with answers: \" + str(len(questions[questions['replies'].map(lambda d: len(d)) > 0]))) \n",
    "\n",
    "q_a_pairs = []\n",
    "for idx, q in questions.iterrows():\n",
    "    if q['replies']:\n",
    "        q_text = df.loc[idx,'full_text']\n",
    "        for a in q['replies']:\n",
    "            a_text = df.loc[a,'full_text']\n",
    "            q_a_pairs.append((q_text,a_text))\n",
    "            \n",
    "print(\"Question-answer pairs: \" + str(len(q_a_pairs)))\n",
    "q_a_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "\n",
    "* Users\n",
    "* Hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_id_str'] = df.apply(lambda x: x['user']['id_str'], axis=1)\n",
    "df['user_id_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# number of replies\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "g = sns.countplot(df.apply(lambda x: len(x['replies']), axis=1))\n",
    "g.set_yscale('log')\n",
    "plt.subplot(1, 2, 2)\n",
    "g = sns.countplot(questions.apply(lambda x: len(x['replies']), axis=1))\n",
    "g.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "g = sns.countplot(df.apply(lambda x: len(x['replies'])>0, axis=1))\n",
    "g.set_yscale('log')\n",
    "plt.subplot(1, 2, 2)\n",
    "g = sns.countplot(questions.apply(lambda x: len(x['replies'])>0, axis=1))\n",
    "g.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often do users contribute in the thread?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.countplot(df['user_id_str'].value_counts())\n",
    "g.set_yscale('log')\n",
    "plt.xlabel(\"times in dataset\") \n",
    "plt.ylabel(\"number of users\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoji\n",
    "\n",
    "Find all tweets with emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "from collections import defaultdict\n",
    "\n",
    "emojitweets = defaultdict(list)\n",
    "\n",
    "def find_emoji(text):\n",
    "    emojis_found=[]\n",
    "    for c in text:\n",
    "        if c in emoji.UNICODE_EMOJI and not c in emojis_found:\n",
    "            emojitweets[c].append(text)\n",
    "            emojis_found.append(c)\n",
    "    return()\n",
    "               \n",
    "for idx, text in df['text_only'].iteritems():\n",
    "    find_emoji(text)\n",
    "\n",
    "emojitweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 4))\n",
    "plt.bar(emojitweets.keys(),[len(x) for x in emojitweets.values()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for emoji in sorted(emojitweets, key=lambda x:len(emojitweets[x]), reverse=True):\n",
    "    print(emoji, len(emojitweets[emoji]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open questions\n",
    "\n",
    "* How are the emojis distributed across levels of discussion? E.g., if there is one emoji in a higher-up message, is it more likely that more emoji will follow?\n",
    "* How are links and hashtags distributed in different branches?\n",
    "* Are there any users who contribute to different sub-branches of the tree?\n",
    "* Does the number of followers a user has influence the probability that their post/questions is answered?\n",
    "\n",
    "(For all these, one should probably exclude the root tweet from consideration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
